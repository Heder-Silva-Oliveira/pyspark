#IMPORTS
from pyspark.sql import SparkSession
from dateutil.relativedelta import relativedelta
from pyspark.sql.window import Window
from pyspark.sql.functions import col, trim, when, upper, regexp_replace, expr, regexp_extract, lit, udf, monotonically_increasing_id, count, sum, current_timestamp, cast, row_number, datediff, format_number
import re
import requests 

from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, DateType , DecimalType, TimestampType, BooleanType, ShortType, DoubleType 
 
spark = SparkSession.builder.appName("VENDAS").getOrCreate()

#FUNÇÕES
def cnpj_valido(cnpj):
    cnpj = re.sub(r'[^0-9]', '', cnpj)
    if len(cnpj) !=14:
        return False
    
    total = 0 
    resto = 0 
    digito_verificador_1 = 0
    digito_verificador_2 = 0
    multiplicadores1 = [5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 2]
    multiplicadores2 = [6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 2]

    for i in range(0,12,1):
        total += int(cnpj[i]) * int(multiplicadores1[i])
    resto = total % 11
    
    if resto < 2:
        digito_verificador_1 = 0
    else:
        digito_verificador_1 = 11 - resto

    total = 0
    resto = 0

    for i in range(0,13,1):
        total += int(cnpj[i]) * int(multiplicadores2[i])

    resto = total % 11
    if resto < 2:
        digito_verificador_2 = 0 
    else:
        digito_verificador_2 = 11 - resto

    return cnpj[-2:] == str(digito_verificador_1) + str(digito_verificador_2)   
cnpj_valido = udf(cnpj_valido, BooleanType())




def limpar_tabela(tabela):
    colunas = tabela.columns
    for coluna in colunas:
        tabela = tabela.withColumn(coluna, trim(col(coluna))) 
    tabela_limpa = tabela.na.drop()
    tabela_limpa_final = tabela_limpa.dropDuplicates()
    #tabela_limpa_final = tabela_duplicados.withColumn("MOTIVO", lit("OK"))

    dados_repetidos = tabela.subtract(tabela_limpa_final)
    dados_repetidos_motivo = dados_repetidos.withColumn("MOTIVO", lit("Dado Repetido"))

    deletados = tabela.subtract (tabela_limpa)
    deletados_motivo = deletados.withColumn("MOTIVO", lit("Dado Nulo"))

    deletados_final = deletados_motivo.union(dados_repetidos_motivo)
    return tabela_limpa_final, deletados_final 


def verifica_cep(cep):
    url = f"https://viacep.com.br/ws/{cep}/json/"
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()
        if "erro" in data:
            return f"NAO ENCONTRADO"
        else:
            return f"OK"
    else:
        return f"ERRO"
verifica_cep_udf = udf(verifica_cep, StringType()) 

def gerar_parcelas(param_nf):
    
    parcelas = spark.createDataFrame([], schema = schemaparcela)

    def calculo(param):
        numero_parcela = 1
        final = spark.createDataFrame([], schema = schemaparcela)
        while numero_parcela <= param['QTD_PARCELAS']:
            data_recebimento = param['DATA_EMISSAO'] + relativedelta(months=(numero_parcela - param['ENTRADA']))
            new_row = spark.createDataFrame([(param['NUMERO_NF'], data_recebimento, (param['VALOR_TOTAL']/param['QTD_PARCELAS']), numero_parcela, 0)], schema = schemaparcela)
            final = final.union(new_row)
            numero_parcela += 1
        return final

    linhas_nf_parcela = param_nf.collect()

    for linha in linhas_nf_parcela:
        parcelas = parcelas.union(calculo(linha))
    
    return parcelas
 

def gerar_divergencias(param_d):

    div = spark.createDataFrame([], schema = schemaDivergencia)
    
    def verificar(param):
        id = 1
        finale = spark.createDataFrame([], schema = schemaDivergencia)
        while id < 2:
            if  (param['DIAS'] == 0) & (param['PORCENTAGEM'] > -0.005) & (param['PORCENTAGEM'] < 0.005): 
                new_row = spark.createDataFrame([(param['NUMERO_NF'], param['DATA_VENCIMENTO'], param['DATA_RECEBIMENTO'], param['VALOR_PARCELA'], param['VALOR_RECEBIDO'], param['NUM_PARCELA'],param['STATUS_RECEBIMENTO'], True)], schema = schemaDivergencia)
            elif(param['DIAS'] >= -10) & (param['DIAS'] <= -6) & (param['PORCENTAGEM'] > -0.03) & (param['PORCENTAGEM'] < -0.02):
                new_row = spark.createDataFrame([(param['NUMERO_NF'], param['DATA_VENCIMENTO'], param['DATA_RECEBIMENTO'], param['VALOR_PARCELA'], param['VALOR_RECEBIDO'], param['NUM_PARCELA'],param['STATUS_RECEBIMENTO'], True)], schema = schemaDivergencia)
            elif(param['DIAS'] >= -5) & (param['DIAS'] <= -1) & (param['PORCENTAGEM'] > -0.019) & (param['PORCENTAGEM'] < -0.01):
                new_row = spark.createDataFrame([(param['NUMERO_NF'], param['DATA_VENCIMENTO'], param['DATA_RECEBIMENTO'], param['VALOR_PARCELA'], param['VALOR_RECEBIDO'], param['NUM_PARCELA'],param['STATUS_RECEBIMENTO'], True)], schema = schemaDivergencia)
            elif(param['DIAS'] <= -11) & (param['PORCENTAGEM'] > -0.05) & (param['PORCENTAGEM'] < -0.0301):
                new_row = spark.createDataFrame([(param['NUMERO_NF'], param['DATA_VENCIMENTO'], param['DATA_RECEBIMENTO'], param['VALOR_PARCELA'], param['VALOR_RECEBIDO'], param['NUM_PARCELA'],param['STATUS_RECEBIMENTO'], True)], schema = schemaDivergencia)
            elif(param['DIAS'] >= 1) & (param['DIAS'] <= 5) & (param['PORCENTAGEM'] > 0.01) & (param['PORCENTAGEM'] < 0.05):
                new_row = spark.createDataFrame([(param['NUMERO_NF'], param['DATA_VENCIMENTO'], param['DATA_RECEBIMENTO'], param['VALOR_PARCELA'], param['VALOR_RECEBIDO'], param['NUM_PARCELA'],param['STATUS_RECEBIMENTO'], True)], schema = schemaDivergencia)
            elif(param['DIAS'] >= 6) & (param['DIAS'] <= 10) & (param['PORCENTAGEM'] > 0.016) & (param['PORCENTAGEM'] < 0.02):
                new_row = spark.createDataFrame([(param['NUMERO_NF'], param['DATA_VENCIMENTO'], param['DATA_RECEBIMENTO'], param['VALOR_PARCELA'], param['VALOR_RECEBIDO'], param['NUM_PARCELA'],param['STATUS_RECEBIMENTO'], True)], schema = schemaDivergencia)
            elif(param['DIAS'] >= 11) & (param['PORCENTAGEM'] > 0.021):
                new_row = spark.createDataFrame([(param['NUMERO_NF'], param['DATA_VENCIMENTO'], param['DATA_RECEBIMENTO'], param['VALOR_PARCELA'], param['VALOR_RECEBIDO'], param['NUM_PARCELA'],param['STATUS_RECEBIMENTO'], True)], schema = schemaDivergencia)
            else:
                new_row = spark.createDataFrame([(param['NUMERO_NF'], param['DATA_VENCIMENTO'], param['DATA_RECEBIMENTO'], param['VALOR_PARCELA'], param['VALOR_RECEBIDO'], param['NUM_PARCELA'],param['STATUS_RECEBIMENTO'], False)], schema = schemaDivergencia)
            finale = finale.union(new_row)
            id += 1
        return finale
        

    linhas_div = param_d.collect()
    
    for l in linhas_div:
        div = div.union(verificar(l))
    
    return div
 

schemaCEP = StructType([
    StructField("CEP", IntegerType(), True),
    StructField("UF", StringType(), True),
    StructField("CIDADE", StringType(), True),
    StructField("BAIRRO", StringType(), True),
    StructField("LOGRADOURO", StringType(), True)
])

schemaCliente = StructType([
    StructField("ID", IntegerType(), False),
    StructField("NOME", StringType(), False),
    StructField("CNPJ", StringType(), False),
    StructField("EMAIL", StringType(), False),
    StructField("TELEFONE", StringType(), False)
])

schemaCondicaoPagamento = StructType([
    StructField("ID", IntegerType(), False),
    StructField("DESCRICAO", StringType(), False),
    StructField("QTD_PARCELAS", IntegerType(), False),
    StructField("ENTRADA", ShortType(), False)
])

schemaEnderecoCliente = StructType([
    StructField("ID", IntegerType(), False),
    StructField("ID_CLIENTE", StringType(), False),
    StructField("ID_TIPO_ENDERECO", StringType(), False),
    StructField("CEP", IntegerType(), False),
    StructField("NUMERO", IntegerType(), False),
    StructField("COMPLEMENTO", StringType(), True)
])

schemaHistoricoRecebimento = StructType([
    StructField("ID", IntegerType(), False),
    StructField("ID_PROGRAMACAO_RECEBIMENTO", IntegerType(), False),
    StructField("ID_DESCONTO", IntegerType(), False),
    StructField("DATA_RECEBIMENTO", DateType(), False),
    StructField("VALOR_ATIVO", DecimalType(8,2), False),
    StructField("VALOR_PAGO", DecimalType(8,2), False)
])

schemaHistoricoRecebimentoDivergente = StructType([
    StructField("ID", IntegerType(), False),
    StructField("ID_PROGRAMACAO_RECEBIMENTO", IntegerType(), False),
    StructField("ID_DESCONTO", IntegerType(), False),
    StructField("DATA_RECEBIMENTO", DateType(), False),
    StructField("VALOR_ATIVO", DecimalType(8,2), False),
    StructField("VALOR_PAGO", DecimalType(8,2), False),
    StructField("MOTIVO", StringType(), False)
])

schemaNotaFiscalSaida = StructType([
    StructField("ID", IntegerType(), False),
    StructField("ID_CLIENTE", IntegerType(), False),
    StructField("ID_CONDICAO", IntegerType(), False),
    StructField("NUMERO_NF", IntegerType(), False),
    StructField("DATA_EMISSAO", DateType(), False),
    StructField("VALOR_NET", DecimalType(8,2), False),
    StructField("VALOR_TRIBUTO", DecimalType(8,2), False),
    StructField("VALOR_TOTAL", DecimalType(8,2), False),
    StructField("NOME_ITEM", StringType(), False),
    StructField("QTD_ITEM", IntegerType(), False)
])
schemaProgramacaoRecebimento = StructType([
    StructField("ID", IntegerType(), False),
    StructField("ID_NF_ENTRADA", IntegerType(), False),
    StructField("DATA_VENCIMENTO", DateType(), False),
    StructField("NUM_PARCELA", IntegerType(), False),
    StructField("VALOR_PARCELA", DecimalType(8,2), False),
    StructField("STATUS_RECEBIMENTO", ShortType(), False)
])

#CRIAÇÃO TODAS AS TABELAS NECESSARIAS   S T A G E 
schemaVendas = StructType([
    StructField("NOME_CLIENTE", StringType(), True),
    StructField("CNPJ_CLIENTE", StringType(), True),
    StructField("EMAIL_CLIENTE", StringType(), True),
    StructField("TELEFONE_CLIENTE", StringType(), True),
    StructField("NUMERO_NF", LongType(), True),
    StructField("DATA_EMISSAO", DateType(), True),
    StructField("VALOR_NET", DecimalType(8,2), True),
    StructField("VALOR_TRIBUTO", DecimalType(8,2), True),
    StructField("VALOR_TOTAL", DecimalType(8,2), True),
    StructField("NOME_ITEM", StringType(), True),
    StructField("QTD_ITEM", IntegerType(), True),
    StructField("CONDICAO_PAGAMENTO", StringType(), True),
    StructField("CEP", IntegerType(), True),
    StructField("NUM_ENDERECO", IntegerType(), True),
    StructField("COMPLEMENTO", StringType(), True),
    StructField("TIPO_ENDERECO", StringType(), True),
    StructField("DATA_PROCESSAMENTO", DateType(), True)
])

schemaRecebimento = StructType([
    StructField("NUMERO_NF", IntegerType(), True),
    StructField("VALOR_RECEBIDO", DecimalType(8,2), True),
    StructField("DATA_VENCIMENTO", DateType(), True),
    StructField("DATA_RECEBIMENTO", DateType(), True),
    StructField("DATA_PROCESSAMENTO", DateType(), True)
])

schemaPagamento = StructType([
    StructField("NUMERO_NF", IntegerType(), True),
    StructField("DATA_VENCIMENTO", DateType(), True),
    StructField("DATA_PAGAMENTO", DateType(), True),
    StructField("VALOR_PAGO", DecimalType(8,2), True)
])

schemaDivergencia = StructType([
                    StructField("NUMERO_NF", IntegerType(), True),
                    StructField("DATA_RECEBIMENTO", DateType(), True),
                    StructField("DATA_RECEBIMENTO_EFETUADO", DateType(), True),
                    StructField("VALOR_PARCELA", DecimalType(8,2), True),
                    StructField("VALOR_PARCELA_RECEBIDA", DecimalType(8,2), True),
                    StructField("NUM_PARCELA", ShortType(), True),
                    StructField("STATUS", ShortType(), True),
                    StructField("VERIFICACAO", BooleanType(), True)
])

schemaparcela = StructType([
                    StructField("NUMERO_NF", IntegerType(), True),
                    StructField("DATA_VENCIMENTO", DateType(), True),
                    StructField("VALOR_PARCELA", DecimalType(8,2), True),
                    StructField("NUM_PARCELA", ShortType(), True),
                    StructField("STATUS", ShortType(), True)
])

#T I P O _ E N D E R E C O ---------------------------------------------------------------------------------------------------------------------------------------------------------
tipo_endereco = spark.read.options(header='True').schema(schemaTipoEndereco).csv('/home/pedotti/Documents/MeuProjetoSpark/arquivos_csv/tipo_endereco.csv')

#T I P O _ D E S C O N T O ---------------------------------------------------------------------------------------------------------------------------------------------------------
tipo_desconto = spark.read.options(header='True').schema(schemaTipoEndereco).csv('/home/pedotti/Documents/MeuProjetoSpark/arquivos_csv/tipo_desconto.csv')

#C O N D I C A O   P A G A M E N T O ---------------------------------------------------------------------------------------------------------------------------------------------------------
condicao_pagamento = spark.read.options(header='True').schema(schemaCondicaoPagamento).csv('/home/pedotti/Documents/MeuProjetoSpark/arquivos_csv/condicao_pagamento.csv')
 
#C E P ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
cep = spark.read.options(header='True', delimiter=';').schema(schemaCEP).csv('/home/pedotti/Documents/MeuProjetoSpark/arquivos_csv/CEP_NEW.csv')
cep_limpo, CEP_deletado= limpar_tabela(cep)

#verificar cep tem 8 digitos
cep_verificado = cep_limpo.filter(regexp_extract(col("CEP"), r'^\d{8}$', 0) != "")
#chamada da funcao na api dos IBGE
cep_final = cep_verificado.withColumn("VALIDACAO", verifica_cep_udf(col("CEP")))

cep_final = cep_verificado.withColumn("CEP", col("CEP").cast("integer")).select("CEP", "UF", "CIDADE", "BAIRRO", "LOGRADOURO")
cep_final = spark.createDataFrame(cep_final.collect(), schema = schemaCEP)


#cep_final.show()
print('cep')

#V E N D A S ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
vendas = spark.read.options(header='True').schema(schemaVendas).csv('/home/pedotti/Documents/MeuProjetoSpark/arquivos_csv/vendas.csv')

#completando nulos coluna complemento
vendas_complemento = vendas.withColumn("COMPLEMENTO", when(col("COMPLEMENTO").isNull(),"N/A").otherwise(col("COMPLEMENTO")))

#executando função de limpar nulos e dar trim em todas as colunas 
vendas_limpas, vendas_deletado = limpar_tabela(vendas_complemento)

#limpando cnpj invalido
vendas_cnpj = vendas_limpas.withColumn("CNPJ_VALIDO", cnpj_valido("CNPJ_CLIENTE"))
#vendas_deletado_cnpj = vendas_deletado.withColumn("CNPJ_VALIDO", cnpj_valido("CNPJ_CLIENTE"))

vendas_cnpj_validado = vendas_cnpj.where(vendas_cnpj["CNPJ_VALIDO"] == True)
vendas_cnpj_errado = vendas_cnpj.where(vendas_cnpj["CNPJ_VALIDO"] == False)
#vendas_deletado_final = vendas_deletado_cnpj.union(vendas_cnpj_errado)


#nome fornecedor maiusculo
vendas_validado_final = vendas_cnpj_validado.withColumn("NOME_CLIENTE", upper(col("NOME_CLIENTE"))).withColumn("CONDICAO_PAGAMENTO",
                            when(col("CONDICAO_PAGAMENTO").substr(2, 4).like("%ntra%"),
                                 col("CONDICAO_PAGAMENTO"))
                            .otherwise(
                                when(col("CONDICAO_PAGAMENTO").like("%90 dias") |
                                     col("CONDICAO_PAGAMENTO").like("%noventa dias"),
                                     "30/60/90 dias")
                                .when(col("CONDICAO_PAGAMENTO").like("%60 dias"),
                                      "30/60 dias")
                                .when(col("CONDICAO_PAGAMENTO").like("%vista"),
                                      "A vista")
                                .otherwise(col("CONDICAO_PAGAMENTO"))
                            ))

        

vendas_validado_final = vendas_validado_final.drop('CNPJ_VALIDO')
vendas_validado_final = vendas_validado_final.withColumn("NUMERO_NF", col("NUMERO_NF").cast("long")).withColumn("DATA_EMISSAO", col("DATA_EMISSAO").cast("date")).withColumn("VALOR_NET", col("VALOR_NET").cast("decimal(8,2)")).withColumn("VALOR_TRIBUTO", col("VALOR_TRIBUTO").cast("decimal(8,2)")).withColumn("VALOR_TOTAL", col("VALOR_TOTAL").cast("decimal(8,2)")).withColumn("QTD_ITEM", col("QTD_ITEM").cast("integer")).withColumn("CEP", col("CEP").cast("integer")).withColumn("NUM_ENDERECO", col("NUM_ENDERECO").cast("integer")).withColumn("DATA_PROCESSAMENTO", col("DATA_PROCESSAMENTO").cast("date"))

vendas_validado_final = spark.createDataFrame(vendas_validado_final.collect(), schema = schemaVendas)        

vendas_validado_final.show(1)   

#C L I E N T E ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
cliente = vendas_validado_final.select("NOME_CLIENTE", "CNPJ_CLIENTE", "EMAIL_CLIENTE", "TELEFONE_CLIENTE").distinct().withColumn("ID", monotonically_increasing_id() + 1)
cliente = cliente.withColumn("ID", col("ID").cast("integer")).withColumn("CNPJ_CLIENTE", col("CNPJ_CLIENTE").cast("long")).select("ID","NOME_CLIENTE", "CNPJ_CLIENTE", "EMAIL_CLIENTE", "TELEFONE_CLIENTE")
cliente_final = spark.createDataFrame(cliente.collect(), schema = schemaCliente)
cliente_final.show(1)

#E N D E R E C O   C L I E N T E ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
tipo_endereco_temp = tipo_endereco.withColumnRenamed("DESCRICAO", "TIPO_ENDERECO")
endereco_cliente = vendas_validado_final.withColumnRenamed("CNPJ_CLIENTE", "CNPJ").join(cliente_final, "CNPJ", "right").select("ID", "TIPO_ENDERECO", "CNPJ", "CEP", "NUM_ENDERECO", "COMPLEMENTO").distinct().withColumn("VALIDACAO", verifica_cep_udf(col("CEP"))).withColumnRenamed("ID", "ID_CLIENTE")
tipo_endereco_temp = tipo_endereco_temp.withColumn("TIPO_ENDERECO", upper(tipo_endereco_temp["TIPO_ENDERECO"]))

endereco_cliente_cep_errado = endereco_cliente.filter(endereco_cliente["VALIDACAO"] != "OK")
endereco_cliente = endereco_cliente.filter(endereco_cliente["VALIDACAO"] == "OK").join(tipo_endereco_temp, "TIPO_ENDERECO", "left").select("ID_CLIENTE","ID", "CEP", "NUM_ENDERECO", "COMPLEMENTO")
endereco_cliente = endereco_cliente.withColumnRenamed("ID", "ID_TIPO_ENDERECO")
endereco_cliente = endereco_cliente.withColumn("ID", monotonically_increasing_id() + 1)
endereco_cliente = endereco_cliente.withColumn("ID", col("ID").cast("integer")).withColumn("NUM_ENDERECO", col("NUM_ENDERECO").cast("integer")).withColumn(("CEP"), col("CEP").cast("integer")).select("ID", "ID_CLIENTE", "ID_TIPO_ENDERECO", "CEP", "NUM_ENDERECO", "COMPLEMENTO")

endereco_cliente_final = spark.createDataFrame(endereco_cliente.collect(), schema = schemaEnderecoCliente)
#endereco_cliente.display()
#endereco_cliente.printSchema()
endereco_cliente_final.show(1)


#N O T A   S A I D A ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
condicao_pagamento_temp = condicao_pagamento.withColumnRenamed("DESCRICAO", "CONDICAO_PAGAMENTO")
nf_saida = vendas_validado_final.withColumnRenamed("CNPJ_CLIENTE","CNPJ").join(cliente_final, "CNPJ", "left").withColumnRenamed("ID", "ID_CLIENTE")

nf_saida = nf_saida.join(condicao_pagamento_temp, "CONDICAO_PAGAMENTO", 'left').withColumnRenamed("ID", "ID_CONDICAO_PAGAMENTO")

nf_saida_1, nf_saida_deletados = limpar_tabela(nf_saida)

nf_saida_1 = nf_saida_1.withColumn("ID", monotonically_increasing_id() + 1).withColumn("ID", col("ID").cast("integer")).withColumn("ID_CLIENTE", col("ID_CLIENTE").cast("integer")).withColumn("ID_CONDICAO_PAGAMENTO", col("ID_CONDICAO_PAGAMENTO").cast("integer")).withColumn("NUMERO_NF", col("NUMERO_NF").cast("integer")).withColumn("DATA_EMISSAO", col("DATA_EMISSAO").cast("date")).withColumn("VALOR_NET", col("VALOR_NET").cast("decimal(8,2)")).withColumn("VALOR_TRIBUTO", col("VALOR_TRIBUTO").cast("decimal(8,2)")).withColumn("VALOR_TOTAL", col("VALOR_TOTAL").cast("decimal(8,2)")).withColumn("QTD_ITEM", col("QTD_ITEM").cast("integer")).withColumn("QTD_PARCELAS", col("QTD_PARCELAS").cast("integer")).withColumn("ENTRADA", col("ENTRADA").cast("integer"))

nf_saida_2 = nf_saida_1.select("ID_CLIENTE", "ID_CONDICAO_PAGAMENTO", "NUMERO_NF", "DATA_EMISSAO", "VALOR_NET", "VALOR_TRIBUTO", "VALOR_TOTAL", "NOME_ITEM", "QTD_ITEM", "QTD_PARCELAS", "ENTRADA")

nf_saida_final = nf_saida_1.select("ID", "ID_CLIENTE", "ID_CONDICAO_PAGAMENTO", "NUMERO_NF", "DATA_EMISSAO", "VALOR_NET", "VALOR_TRIBUTO", "VALOR_TOTAL", "NOME_ITEM", "QTD_ITEM")

nf_saida_final = spark.createDataFrame(nf_saida_final.collect(), schema = schemaNotaFiscalSaida)

nf_saida_final.show(1)

#P R O G R A M A C A O    R E C E B I M E N T O ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
nf_saida_2 = nf_saida_2.select("NUMERO_NF", "DATA_EMISSAO", "VALOR_TOTAL", "QTD_PARCELAS", "ENTRADA")

window_spec = Window.orderBy('NUMERO_NF')
programacao_recebimento = gerar_parcelas(nf_saida_2)

programacao_recebimento = programacao_recebimento.join(nf_saida_final, "NUMERO_NF", "inner").withColumnRenamed("ID", "ID_NF_ENTRADA")

programacao_recebimento = programacao_recebimento.withColumn("ID", row_number().over(window_spec)).withColumn("ID", col("ID").cast("integer"))

programacao_recebimento = programacao_recebimento.select("ID", "ID_NF_ENTRADA", "DATA_VENCIMENTO", "NUM_PARCELA", "VALOR_PARCELA", "STATUS")

programacao_recebimento_final = spark.createDataFrame(programacao_recebimento.collect(), schema = schemaProgramacaoRecebimento)

programacao_recebimento_final.show(1)


#R E C E B I M E N T O ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
recebimento = spark.read.options(header='True').csv('/home/pedotti/Documents/MeuProjetoSpark/arquivos_csv/recebimentos.csv')
recebimento_limpo, recebimento_deletado = limpar_tabela(recebimento)
recebimento_limpo = recebimento_limpo.withColumn("NUMERO_NF", col("NUMERO_NF").cast("integer")).withColumn("VALOR_PARCELA_RECEBIDA", col("VALOR_PARCELA_RECEBIDA").cast("decimal(8,2)")).withColumn("DATA_RECEBIMENTO", col("DATA_RECEBIMENTO").cast("date")).withColumn("DATA_RECEBIMENTO_EFETUADO", col("DATA_RECEBIMENTO_EFETUADO").cast("date")).withColumn("DATA_PROCESSAMENTO", col("DATA_PROCESSAMENTO").cast("date")).select("NUMERO_NF", "VALOR_PARCELA_RECEBIDA", "DATA_RECEBIMENTO", "DATA_RECEBIMENTO_EFETUADO", "DATA_PROCESSAMENTO")

recebimento_final = spark.createDataFrame(recebimento_limpo.collect(), schema = schemaRecebimento)

recebimento_final.show(1)


#H I S T O R I C O   R E C E B I M E N T O ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

programacao_recebimento_final_1 = programacao_recebimento_final.join(nf_saida_final, [programacao_recebimento_final.ID_NF_ENTRADA == nf_saida_final.ID], "inner").select(programacao_recebimento_final["ID"], "NUMERO_NF", "DATA_VENCIMENTO", "NUM_PARCELA", "VALOR_PARCELA", "STATUS_RECEBIMENTO")

historico_recebimento_1 = programacao_recebimento_final_1.join(recebimento_final, [programacao_recebimento_final_1.NUMERO_NF == recebimento_final.NUMERO_NF, programacao_recebimento_final_1.DATA_VENCIMENTO == recebimento_final.DATA_VENCIMENTO, programacao_recebimento_final_1.STATUS_RECEBIMENTO == 0], "inner").select(programacao_recebimento_final_1.ID ,programacao_recebimento_final_1.NUMERO_NF, programacao_recebimento_final_1.DATA_VENCIMENTO, recebimento_final.DATA_RECEBIMENTO, programacao_recebimento_final_1.VALOR_PARCELA, recebimento_final.VALOR_RECEBIDO, programacao_recebimento_final_1.NUM_PARCELA,programacao_recebimento_final_1.STATUS_RECEBIMENTO).withColumn("STATUS_RECEBIMENTO", lit(1))

historico_recebimento_1.show(1)

trat_divergencia = historico_recebimento_1.withColumn("DIAS", datediff(historico_recebimento_1["DATA_RECEBIMENTO"], historico_recebimento_1["DATA_VENCIMENTO"]))#.withColumn("PORCENTAGEM", (historico_recebimento_1["VALOR_RECEBIDO"] - historico_recebimento_1["VALOR_PARCELA"])/historico_recebimento_1["VALOR_PARCELA"])

trat_divergencia = trat_divergencia.withColumn("PORCENTAGEM", ((lit(1) - (col("VALOR_PARCELA") / col("VALOR_RECEBIDO"))) * 100)).withColumn("PORCENTAGEM", format_number(col("PORCENTAGEM"), 2)).withColumn("PORCENTAGEM", col("PORCENTAGEM").cast("decimal(4,2)"))

trat_divergencia.show(1)


historico_divergencia = spark.createDataFrame([], schema = schemaDivergencia)
historico_divergencia = historico_divergencia.union(gerar_divergencias(trat_divergencia)).filter(col("VERIFICACAO") == False).select("NUMERO_NF", "DATA_RECEBIMENTO", "DATA_RECEBIMENTO_EFETUADO", "VALOR_PARCELA", "VALOR_PARCELA_RECEBIDA", "NUM_PARCELA", "STATUS")



historico_divergencia.show(1)

